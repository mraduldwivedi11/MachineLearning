{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from pprint import pprint\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching 20 news group data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset=\"train\", shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check most occured 2000 words\n",
    "\n",
    "dictionary={}\n",
    "\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "\n",
    "for i in range(0,len(twenty_train['data'])):  # to traverse from all doc\n",
    "    strn=re.sub(r'[^\\w\\s]','',twenty_train['data'][i])  #regular expressions to remove punctuation words and digit words\n",
    "    strn=re.sub(r'\\w*\\d\\w*', '', strn).strip()\n",
    "    shortword.sub('', strn)\n",
    "    word_tokens = strn.split(\" \")                 # split whole text into space separted words\n",
    "    word_tokens = [word.lower() for word in word_tokens] # convert it into lower cases\n",
    "    for w in word_tokens:\n",
    "        if w.isalpha():                                   # to check if that word is alphanumeric\n",
    "            if w not in stop_word:\n",
    "                if len(w)>2:\n",
    "                    if w in dictionary:                   # to chech if it is already in dic so increment its occurenc\n",
    "                        dictionary[w]=dictionary[w]+1\n",
    "                    else:\n",
    "                        dictionary[w]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to sort that words according to occurance\n",
    "features = sorted(dictionary.items(), key=operator.itemgetter(1),reverse=True)[0:2000] # and do slicing for 2000 word\n",
    "features_list=[]\n",
    "for i in features:\n",
    "    features_list.append(i[0])   ## append every feature tuple's 0'th element into feature list\n",
    "#pprint(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(0, index=range(0,len(twenty_train['data'])), columns=range(0,2000))\n",
    "                            ## create a empty dataframe initialize from 0 of shape (11314, 2000) \n",
    "                            \n",
    "    \n",
    "## in below loop we traverse from all doc and convert it into words and put our selected word features in new row for \n",
    "# every new doc\n",
    "    \n",
    "for i in range(0,len(twenty_train['data'])):\n",
    "        word_tokens = twenty_train['data'][i].split(\" \")   ## convert one doc into words\n",
    "        word_tokens = [word.lower() for word in word_tokens]\n",
    "        for word in word_tokens:\n",
    "            if word in features_list:                         # check if that word is in our feature words\n",
    "                column_no=features_list.index(word)\n",
    "                df.iloc[i,column_no]=df.iloc[i,column_no]+1   # increments it's occurance by one\n",
    "            else:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(twenty_train['target']))\n",
    "# print(df.shape)\n",
    "# twenty_train['target'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert our dataframe into numpy\n",
    "x=df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(x,twenty_train['target'],test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train our classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "rec.motorcycles\n"
     ]
    }
   ],
   "source": [
    "single_predict=clf.predict(X_test[52].reshape(1, -1))\n",
    "print(type(single_predict))\n",
    "print(twenty_train['target_names'][single_predict[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# print(classification_report(Y_test,Y_pred))\n",
    "# print(confusion_matrix(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_of_dataframe(X_train, Y_train):\n",
    "    result = {}\n",
    "    class_values = set(Y_train)\n",
    "    for current_class in class_values:\n",
    "        result[current_class] = {}\n",
    "        result[\"total_data\"] = len(Y_train)\n",
    "        current_class_rows = (Y_train == current_class)\n",
    "        X_train_current = X_train[current_class_rows]\n",
    "        Y_train_current = Y_train[current_class_rows]\n",
    "        num_features = X_train.shape[1]\n",
    "        result[current_class][\"total_count\"] = len(Y_train_current)\n",
    "        for j in range(1, num_features + 1):\n",
    "            result[current_class][j] = {}\n",
    "            all_possible_values = set(X_train[:, j - 1])\n",
    "            for current_value in all_possible_values:\n",
    "                result[current_class][j][current_value] = (X_train_current[:, j - 1] == current_value).sum()\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dictionary=create_dictionary_of_dataframe(X_train , Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "pprint(df_dictionary[0][3].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(df_dictionary, x, current_class):\n",
    "    output = np.log(df_dictionary[current_class][\"total_count\"]) - np.log(df_dictionary[\"total_data\"])\n",
    "    num_features = len(df_dictionary[current_class].keys()) - 1;\n",
    "    for j in range(1, num_features + 1):\n",
    "        xj = x[j - 1]\n",
    "#          print(j,end=(\",\"))\n",
    "#         print(current_class,j,xj)\n",
    "        count_current_class_with_value_xj = df_dictionary[current_class][j][xj] + 1\n",
    "        print(df_dictionary[current_class][j][xj])\n",
    "        count_current_class = df_dictionary[current_class][\"total_count\"] + len(df_dictionary[current_class][j].keys())\n",
    "        current_xj_probablity = np.log(count_current_class_with_value_xj) - np.log(count_current_class)\n",
    "        output = output + current_xj_probablity\n",
    "#     print()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(df_dictionary, x):\n",
    "    classes = df_dictionary.keys()\n",
    "    best_p = -1000\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    for current_class in classes:\n",
    "        if (current_class == \"total_data\"):\n",
    "            continue\n",
    "        p_current_class = probability(df_dictionary, x, current_class)\n",
    "        \n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "        #print(twenty_train['target_names'][best_class])\n",
    "#         print(best_class)\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df_dictionary, X_test):\n",
    "    y_pred = []\n",
    "#     i=0\n",
    "    for x in X_test:\n",
    "        x_class = predictSinglePoint(df_dictionary, x)\n",
    "        y_pred.append(x_class)\n",
    "#         print(i,end=\",\")\n",
    "#         i=i+1\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_self = predict(df_dictionary,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
